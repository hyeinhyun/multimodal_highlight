{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.printable\n",
    "n_letters = len(all_letters)\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "def linesToTensor(lines):\n",
    "    line_length = 15000\n",
    "    if max([ len(line) for line in lines]) < line_length:\n",
    "        line_length = max( [len(line) for line in lines] )\n",
    "    #line_length = max( [len(line) for line in lines] )\n",
    "    #xx = [max(len(line)-15000,0) for line in lines]\n",
    "    #print float(np.sum(xx)) / float(np.sum([len(line) for line in lines]))\n",
    "    tensor = torch.zeros(len(lines), line_length, n_letters)\n",
    "    for b, line in enumerate(lines): \n",
    "        line = line[:15000]\n",
    "        for li, letter in enumerate(line):\n",
    "            tensor[b][li + line_length - len(line)][letterToIndex(letter)] = 1\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModel(nn.Module):\n",
    "    def __init__(self, preTrained='True', input=100):\n",
    "        super(LangModel, self).__init__()\n",
    "\n",
    "        # Language Model\n",
    "        self.lang = nn.LSTM(input, 128, 3, batch_first=True) \n",
    " \n",
    "        # Output \n",
    "        self.output = nn.Linear(128, 2)\n",
    "        n = self.output.in_features * self.output.out_features\n",
    "        self.output.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        self.output.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        text.cuda()\n",
    "        h0 = ( Variable(torch.zeros(3, text.size(0), 128)).cuda(),  Variable(torch.zeros(3, text.size(0), 128)).cuda())\n",
    "\n",
    "        lang_feature, hn = self.lang(text, h0 )\n",
    "        lang_feature = lang_feature[:,-1,:]\n",
    "\n",
    "        pred = self.output(lang_feature)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-75b211ab24dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchat_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./exp_chat/{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchat_arr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwindow_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'game_id' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "chat_data='./exp_chat/{}.npy'.format(game_id)\n",
    "chat_arr=np.load(chat_data)\n",
    "window_text=[]\n",
    "for i in range(len(chat_arr)):\n",
    "    temp=''\n",
    "    for idx in range(7):\n",
    "        if i+idx<len(chat_arr):\n",
    "            temp+=chat_arr[i+idx]\n",
    "    window_text.append(temp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=linesToTensor(window_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2056, 2700, 100])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class chat_ds(data.Dataset):\n",
    "    def __init__(self,game_id):\n",
    "        self.window_text=[]\n",
    "        #####game id 전체로 바꿀것##############\n",
    "        chat_data='./exp_chat/{}.npy'.format(game_id)\n",
    "        chat_arr=np.load(chat_data)\n",
    "        #window_text=np.zeros([len(text)])\n",
    "        for i in range(len(chat_arr)):\n",
    "            temp=''\n",
    "            for idx in range(7):\n",
    "                if i+idx<len(chat_arr):\n",
    "                    temp+=chat_arr[i+idx]\n",
    "            self.window_text.append(temp)\n",
    "        self.text=linesToTensor(self.window_text)\n",
    "\n",
    "        with open('../tsvt/ocr/hi_time.json','r') as f:\n",
    "            labels=json.load(f)\n",
    "\n",
    "        label=labels[game_id]\n",
    "        \n",
    "        self.label_gt=torch.zeros((len(self.text),))\n",
    "        for (x,y) in label:\n",
    "            start=int(x.split(':')[0])*60+int(x.split(':')[1])\n",
    "            end=int(y.split(':')[0])*60+int(y.split(':')[1])\n",
    "            for i in range(start,end+1):\n",
    "                self.label_gt[i]=1\n",
    "        self.label_gt=self.label_gt.long()\n",
    "    def __len__(self):\n",
    "        return len(self.label_gt)\n",
    "    def __getitem__(self,index):\n",
    "        return self.text[index],self.label_gt[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id='102844212428895431'\n",
    "a=chat_ds(game_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=torch.utils.data.DataLoader(a,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### model load #####\n",
    "model=LangModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "923 1133 0\n",
      "[20/21], prec:0.4489299610894942, recall:1.0, f1:61.967103054716354\n",
      "923 1133 0\n",
      "[20/21], prec:0.4489299610894942, recall:1.0, f1:61.967103054716354\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-a1f189268d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train set\n",
    "for iter in range(10):\n",
    "    model.train()\n",
    "    for inputs,labels in (loader):\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), 0.1)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(inputs)\n",
    "\n",
    "        loss=criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #validation\n",
    "    model.eval()\n",
    "    pred_sum = 0#model output\n",
    "    gt_sum = 0#label\n",
    "    tp_sum=0\n",
    "    fp_sum=0\n",
    "    fn_sum=0\n",
    "    with torch.no_grad():\n",
    "        for it, (inputs,labels) in enumerate(loader):\n",
    "            count+=1\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            output=model(inputs)\n",
    "\n",
    "            TP,FP,TN,FN,pred_len, gt_len=fmeasure(output.cpu(),labels.cpu())\n",
    "            tp_sum += TP\n",
    "            fp_sum += FP\n",
    "            fn_sum += FN\n",
    "            pred_sum += pred_len\n",
    "            gt_sum += gt_len\n",
    "\n",
    "        precision = tp_sum/(tp_sum+fp_sum)\n",
    "        recall = tp_sum / (tp_sum+fn_sum)\n",
    "        f1 = (2*precision*recall / (precision + recall)) * 100\n",
    "        print( tp_sum, fp_sum, fn_sum)\n",
    "        print('[{}/{}], prec:{}, recall:{}, f1:{}'.format(it, len(loader), precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmeasure(output, target):\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.view(-1,1)\n",
    "    target = target.view(-1,1)\n",
    "\n",
    "    #overlap = ((pred== 1) + (target == 1)).gt(1)\n",
    "    #overlap = overlap.view(-1,1)\n",
    "    TP = len(np.where((pred==1)&(target==1)==True)[0]) # True positive\n",
    "    FP = len(np.where((pred==1)&(target==0)==True)[0]) # Condition positive = TP + FN\n",
    "    TN = len(np.where((pred==0)&(target==0)==True)[0])\n",
    "    FN = len(np.where((pred==0)&(target==1)==True)[0])\n",
    "\n",
    "    \n",
    "    #overlap_len = overlap.data.long().sum()\n",
    "    pred_len = pred.data.long().sum()\n",
    "    gt_len   =  target.data.long().sum()\n",
    "\n",
    "    return TP,FP,TN,FN,pred_len, gt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyein_1.2.0",
   "language": "python",
   "name": "newcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
